{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 1: Data crawler. Massively parallel functions crawling data and storing it in Cloud Object Storage.  Obtain information from web pages or tweets and create a dataset of text data. Use FaaS backend in lithops to launch crawling process over serverless functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "analysis_folder_wg = widgets.Text(value='analysis1', placeholder='')\n",
    "items = [widgets.Text(placeholder='String/hashtag #{}'.format(i)) for i in range(6)]\n",
    "parameterItems = [widgets.Text(placeholder='parameter:value #{}'.format(i)) for i in range(3)]\n",
    "\n",
    "acc = widgets.Accordion(children=[analysis_folder_wg, widgets.HBox(items), widgets.HBox(parameterItems)])\n",
    "acc.set_title(0, 'Folder name in storage to save data')\n",
    "acc.set_title(1, 'Tags to crawl')\n",
    "acc.set_title(2, 'Parameters')\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add more hashtags if the don't fit the boxes manually here:\n",
    "hashtags = []\n",
    "parameters = []\n",
    "for i in items:\n",
    "    if i.value != '':\n",
    "        hashtags.append(i.value)\n",
    "for i in parameterItems:\n",
    "    if i.value != '':\n",
    "        parameters.append(i.value)\n",
    "\n",
    "analysis_folder_name = analysis_folder_wg.value\n",
    "\n",
    "print('Hashtags:', hashtags)\n",
    "print('Parameters:',  parameters)\n",
    "print('Folder:',  analysis_folder_name)\n",
    "\n",
    "# Save them on jupyter kernel so we can retrieve the hashtags on Stage 2 notebook\n",
    "%store analysis_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_snscrape import TwitterSearchScraper\n",
    "\n",
    "def scrapSearch(str_to_search, storage):\n",
    "\n",
    "    tweets = \"\"\n",
    "    counter = 0\n",
    "\n",
    "    for tweet in TwitterSearchScraper(str_to_search).get_items():\n",
    "        tweets = tweets + tweet.to_json() + '\\n'\n",
    "\n",
    "        counter = counter + 1\n",
    "        if counter == 8000:\n",
    "            break\n",
    "\n",
    "    # 'key' is only the sought hashtag, without the excluded ones\n",
    "    storage.put_object(bucket=storage.bucket,\n",
    "                        key=analysis_folder_name+'/'+str_to_search.split()[0],\n",
    "                        body=tweets)\n",
    "\n",
    "    return \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each query will include the current hashtag,\n",
    "# and exclude the hashtags to the right of itself in the array.\n",
    "# This way, we can parallelise the searches, while also ensuring that\n",
    "# if a tweet contains multiple hashtags, one of the queries includes said tweet.\n",
    "# \n",
    "# In this case, the queries will be:\n",
    "# #covid -#sars -#coronavirus, #sars -#coronavirus, #coronavirus\n",
    "#hashtags = ['#covid', '#sars', '#coronavirus']\n",
    "queries = []\n",
    "\n",
    "for i in range(len(hashtags)):\n",
    "    s = ' -'.join(hashtags)\n",
    "    for p in parameters:\n",
    "        s += \" \" + p\n",
    "    queries.append(s)\n",
    "    hashtags.pop(0)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Find tweets since this date\n",
    "range_start = datetime.date(2021, 2, 1)\n",
    "\n",
    "# Interval of time that each function will cover\n",
    "interval = datetime.timedelta(days=15)\n",
    "\n",
    "# How many intervals in total \n",
    "# The search will cover from range_start to range_start + n_intervals*interval\n",
    "n_intervals = 4\n",
    "\n",
    "ranges = ['since:{} until:{}'.format(range_start+x*interval, range_start+(x+1)*interval) for x in range(n_intervals)]\n",
    "\n",
    "queries_w_date_ranges = ['{} {}'.format(q, r) for q in queries for r in ranges]\n",
    "\n",
    "print(queries_w_date_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auDgTfW1NYg2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import lithops\n",
    "\n",
    "with lithops.FunctionExecutor(log_level='debug') as fexec:\n",
    "    all_invocations = fexec.map(scrapSearch, queries_w_date_ranges)\n",
    "    print(fexec.get_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cloudbutton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "8a12c58a8b58ac8253dff9b35a44630ba46221fc60666ce38f2b740db2da3db5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}