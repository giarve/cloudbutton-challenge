{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloudbutton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('.venv')",
      "metadata": {
        "interpreter": {
          "hash": "a4ee58a8dbacfde43774fe6219f7a7b143c13dd0083bd1b680a28b57e0c7f2c5"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "source": [
        "Stage 2:  Data preprocessing stage to produce structured data in csv format also stored in Cloud Object Storage. As columns in the csv file we suggest date, geographic location, url, and sentiment analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lithops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from custom_snscrape.twitter import Tweet\n",
        "\n",
        "def map_preprocess(obj) -> dict:\n",
        "\n",
        "    data_body_str = obj.data_stream.read().decode('utf-8')    \n",
        "    tweet = Tweet.from_json(data_body_str)\n",
        "\n",
        "    if tweet.lang not in ['es', 'en', 'it', 'fr', 'de', 'nl']:\n",
        "        return\n",
        "\n",
        "    classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    sentiment = classifier(tweet.content)\n",
        "\n",
        "    # TODO: Perhaps cast it all to string so we don't have to import the library inside reduce_to_csv\n",
        "    return { 'id': tweet.id,\n",
        "             'user': tweet.user.username,\n",
        "             'content': tweet.content.replace('\\n', ' '), # replace newlines so they don't break the csv\n",
        "             'lang': tweet.lang,\n",
        "             'coordinates': tweet.coordinates,\n",
        "             'retweetCount': tweet.retweetCount,\n",
        "             'likeCount': tweet.likeCount,\n",
        "             'quoteCount': tweet.quoteCount,\n",
        "             'replyCount': tweet.replyCount,\n",
        "             'mentionedUsers': tweet.mentionedUsers,\n",
        "             'outlinks': tweet.outlinks,\n",
        "             'date': tweet.date.isoformat(),\n",
        "             'sentiment': sentiment[0]['label'] }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def reduce_to_csv(results, storage):\n",
        "    \n",
        "    # BUG: Because we are passing dataclasses from a pickled module inside map() function as its results, we must\n",
        "    # import something from the library that contains the dataclasses or lithops won't find it.\n",
        "    # If you don't believe me, remove this line :)\n",
        "    from custom_snscrape.twitter import Tweet\n",
        "    #############################################################################################################\n",
        "\n",
        "    column_names = results[0].keys()\n",
        "    csv_buffer = StringIO()\n",
        "\n",
        "    with StringIO() as csv_buffer:\n",
        "        dict_writer = csv.DictWriter(csv_buffer, column_names)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(results)\n",
        "\n",
        "        storage.put_object(bucket=storage.bucket,\n",
        "                        key='test.csv',\n",
        "                        body=csv_buffer.getvalue())\n",
        "\n",
        "    return \"reduced result to cos://{}/test.csv output and stored it\".format(storage.bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hashtags = []\n",
        "\n",
        "# Retrieve hashtags from Stage 1 notebook\n",
        "%store -r hashtags\n",
        "\n",
        "if len(hashtags) == 0:\n",
        "    print('You have not defined any hashtags yet (use Stage 1 notebook or the \\'hashtags\\' variable directly)')\n",
        "\n",
        "# Prepend bucket name and append / to the hashtags\n",
        "bucket_name = lithops.Storage().bucket\n",
        "data_location = [bucket_name+'/'+htag+'/' for htag in hashtags]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstiNpfTHqPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "7012ba8e-c793-4bc8-fcb8-7d02985a7823",
        "tags": []
      },
      "source": [
        "with lithops.FunctionExecutor(runtime='gilarasa/lithops-cloudbutton-challenge-py3.9:0.5', runtime_memory=2048) as fexec:\n",
        "  # , log_level='debug'\n",
        "\n",
        "  fexec.map_reduce(map_preprocess, data_location, reduce_to_csv)\n",
        "  print(fexec.get_result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}