{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloudbutton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
      "metadata": {
        "interpreter": {
          "hash": "a4ee58a8dbacfde43774fe6219f7a7b143c13dd0083bd1b680a28b57e0c7f2c5"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "source": [
        "Stage 2:  Data preprocessing stage to produce structured data in csv format also stored in Cloud Object Storage. As columns in the csv file we suggest date, geographic location, url, and sentiment analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lithops\n",
        "\n",
        "analysis_folder_name = ''\n",
        "\n",
        "# Retrieve analysis_folder from Stage 1 notebook\n",
        "%store -r analysis_folder_name\n",
        "\n",
        "if len(analysis_folder_name) == 0:\n",
        "    print('You have not defined any folder yet (use Stage 1 notebook or the \\'analysis_folder\\' variable directly)')\n",
        "else:\n",
        "    # Prepend bucket name to the hashtags\n",
        "    bucket_name = lithops.Storage().bucket\n",
        "    data_location = ['cos://'+bucket_name+'/'+analysis_folder_name+'/']\n",
        "    print('Paths to preprocess:', data_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from custom_snscrape.twitter import Tweet\n",
        "\n",
        "def map_preprocess(obj) -> dict:\n",
        "\n",
        "    classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "    to_reduction = []\n",
        "\n",
        "    for data_body_str in obj.data_stream.read().decode('utf-8').splitlines():\n",
        "        tweet = Tweet.from_json(data_body_str)\n",
        "\n",
        "        # Skip tweets with non-supported langs\n",
        "        if tweet.lang not in ['es', 'en', 'it', 'fr', 'de', 'nl']:\n",
        "            continue\n",
        "        \n",
        "        sentiment = classifier(tweet.content)\n",
        "\n",
        "        to_reduction.append({\n",
        "                 'id': tweet.id,\n",
        "                 'user': tweet.user.username,\n",
        "                 'content': tweet.content,\n",
        "                 'lang': tweet.lang,\n",
        "                 'coordinates': tweet.coordinates,\n",
        "                 'retweetCount': tweet.retweetCount,\n",
        "                 'likeCount': tweet.likeCount,\n",
        "                 'quoteCount': tweet.quoteCount,\n",
        "                 'replyCount': tweet.replyCount,\n",
        "                 'date': tweet.date.isoformat(),\n",
        "                 'mentionedUsers': tweet.mentionedUsers,\n",
        "                 'outlinks': tweet.outlinks,\n",
        "                 'sentiment': sentiment[0]['label']\n",
        "            })\n",
        "    \n",
        "    return to_reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def reduce_to_csv(results, storage):\n",
        "    \n",
        "    # BUG: Because we are passing dataclasses from a pickled module inside map() function as its results, we must\n",
        "    # import something from the library that contains the dataclasses or lithops won't find it.\n",
        "    # If you don't believe me, remove this line :)\n",
        "    from custom_snscrape.twitter import Tweet\n",
        "    #############################################################################################################\n",
        "\n",
        "    key = analysis_folder_name + '.csv'\n",
        "\n",
        "    with StringIO() as csv_buffer:\n",
        "        column_names = results[0][0].keys() # all rows should have same headers\n",
        "\n",
        "        dict_writer = csv.DictWriter(csv_buffer, column_names)\n",
        "        dict_writer.writeheader()\n",
        "\n",
        "        for single_map_results in results:\n",
        "            dict_writer.writerows(single_map_results)\n",
        "\n",
        "        storage.put_object(bucket=storage.bucket,\n",
        "                            key=key,\n",
        "                            body=csv_buffer.getvalue())\n",
        "\n",
        "    return 'reduced result and stored it at cos://{}/{}'.format(storage.bucket, key)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstiNpfTHqPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "7012ba8e-c793-4bc8-fcb8-7d02985a7823",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "# Bigger chunksize, longer functions (bigger chances of timeout)\n",
        "# Smaller chunksize, faster execution, more parallelism, more resources consumption\n",
        "object_chunksize = int(0.3 * 1024**2)  # 0.3 MB\n",
        "\n",
        "with lithops.FunctionExecutor(runtime='gilarasa/lithops-cloudbutton-challenge-py3.9:0.5', runtime_memory=2048, log_level='debug') as fexec:\n",
        "  \n",
        "  fexec.map_reduce(map_preprocess, data_location, reduce_to_csv, obj_chunk_size=object_chunksize) \n",
        "  print(fexec.get_result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}