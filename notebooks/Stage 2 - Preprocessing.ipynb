{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloudbutton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.4 64-bit ('.venv')",
      "metadata": {
        "interpreter": {
          "hash": "a4ee58a8dbacfde43774fe6219f7a7b143c13dd0083bd1b680a28b57e0c7f2c5"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "source": [
        "Stage 2:  Data preprocessing stage to produce structured data in csv format also stored in Cloud Object Storage. As columns in the csv file we suggest date, geographic location, url, and sentiment analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lithops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from custom_snscrape.twitter import Tweet\n",
        "\n",
        "def map_preprocess(obj) -> dict:\n",
        "\n",
        "    data_body_str = obj.data_stream.read().decode('utf-8')    \n",
        "    tweet = Tweet.from_json(data_body_str)\n",
        "\n",
        "\n",
        "    # if tweet.lang is not a supported language, translate it:\n",
        "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TranslationPipeline\n",
        "\n",
        "    #nlp = pipeline(\"ner\")\n",
        "    # https://huggingface.co/transformers/task_summary.html#named-entity-recognition\n",
        "\n",
        "\n",
        "    # this model is the best one, I tested the others available and they're quite bad...\n",
        "    classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\") # return_all_scores=True? to make better charts perhaps?\n",
        "    sentiment = classifier(tweet.content)\n",
        "\n",
        "    # if it has any attached image:\n",
        "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.ImageClassificationPipeline\n",
        "    # or text-generation??\n",
        "\n",
        "    # tweet.media.previewUrl if tweet.media and tweet.media.type == 'photo' else None\n",
        "\n",
        "    \n",
        "    # TODO: Think if we should return a list of results (storing scrapped list of tweets inside a single bucket entry) so that a single func \n",
        "    # inferences on multiple tweets\n",
        "\n",
        "    # TODO: Perhaps cast it all to string so we don't have to import the library inside reduce_to_csv\n",
        "    return { 'id': tweet.id,\n",
        "             'user': tweet.user,\n",
        "             'content': tweet.content,\n",
        "             'lang': tweet.lang,\n",
        "             'coordinates': tweet.coordinates,\n",
        "             'retweetCount': tweet.retweetCount,\n",
        "             'likeCount': tweet.likeCount,\n",
        "             'quoteCount': tweet.quoteCount,\n",
        "             'replyCount': tweet.replyCount,\n",
        "             'mentionedUsers': tweet.mentionedUsers,\n",
        "             'outlinks': tweet.outlinks,\n",
        "             'sentiment': sentiment[0]['label'] }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def reduce_to_csv(results, storage):\n",
        "    \n",
        "    # BUG: Because we are passing dataclasses from a pickled module inside map() function as its results, we must\n",
        "    # import something from the library that contains the dataclasses or lithops won't find it.\n",
        "    # If you don't believe me, remove this line :)\n",
        "    from custom_snscrape.twitter import Tweet\n",
        "    #############################################################################################################\n",
        "\n",
        "    column_names = results[0].keys()\n",
        "    csv_buffer = StringIO()\n",
        "\n",
        "    with StringIO() as csv_buffer:\n",
        "        dict_writer = csv.DictWriter(csv_buffer, column_names)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(results)\n",
        "\n",
        "        storage.put_object(bucket=storage.bucket,\n",
        "                        key='test.csv',\n",
        "                        body=csv_buffer.getvalue())\n",
        "\n",
        "    return \"reduced result to cos://{}/test.csv output and stored it\".format(storage.bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hashtags = []\n",
        "\n",
        "# Retrieve hashtags from Stage 1 notebook\n",
        "%store -r hashtags\n",
        "\n",
        "if len(hashtags) == 0:\n",
        "    print('You have not defined any hashtags yet (use Stage 1 notebook or the \\'hashtags\\' variable directly)')\n",
        "\n",
        "# Prepend bucket name and append / to the hashtags\n",
        "bucket_name = lithops.Storage().bucket\n",
        "data_location = [bucket_name+'/'+htag+'/' for htag in hashtags]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstiNpfTHqPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "7012ba8e-c793-4bc8-fcb8-7d02985a7823",
        "tags": []
      },
      "source": [
        "with lithops.FunctionExecutor(runtime='gilarasa/lithops-cloudbutton-challenge-py3.9:0.5', runtime_memory=2048) as fexec:\n",
        "  # , log_level='debug'\n",
        "\n",
        "  #  TODO: should aggregate in chunks the tweets so that a single function can preprocess multiple tweets\n",
        "\n",
        "  fexec.map_reduce(map_preprocess, data_location, reduce_to_csv) # obj_chunk_number is used to divide the input string in chunks, so not to be used unless we store more than one tweet in each object (a list of tweets)\n",
        "  print(fexec.get_result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}