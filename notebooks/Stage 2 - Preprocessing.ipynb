{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloudbutton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "source": [
        "Stage 2:  Data preprocessing stage to produce structured data in csv format also stored in Cloud Object Storage. As columns in the csv file we suggest date, geographic location, url, and sentiment analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lithops\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/lithops-cloud/lithops/blob/master/docs/data_processing.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_preprocess(obj):\n",
        "\n",
        "    # TODO: Investigate why this is not generating any logs...\n",
        "\n",
        "    print('Bucket: {}'.format(obj.bucket))\n",
        "    print('Key: {}'.format(obj.key))\n",
        "    print('Partition num: {}'.format(obj.part))\n",
        "\n",
        "    data_body_str = obj.data_stream.read().decode('utf-8')\n",
        "\n",
        "    # TODO: Serialize tweet dataclass from str dict in data_body HERE\n",
        "\n",
        "\n",
        "    # this model is the best one, I tested the others available and they're quite bad...\n",
        "    #classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    #results = classifier(tweet.content)\n",
        "    return data_body_str #results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reduce_to_csv(results):\n",
        "    # TODO: Save results to a single csv with columns as the first row\n",
        "\n",
        "    print(results)\n",
        "\n",
        "    return \"reduced result to csv output and stored it\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstiNpfTHqPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "7012ba8e-c793-4bc8-fcb8-7d02985a7823",
        "tags": []
      },
      "source": [
        "data_location = ['colab/#coronavirus/', 'colab/#sars/', 'colab/#covid/'] # The / at the end is important TODO: Avoid repetition?\n",
        "\n",
        "with lithops.FunctionExecutor() as fexec: #runtime='gilarasa/lithops-cloudbutton-challenge-py3.9:0.1', runtime_memory=2048) as fexec:\n",
        "\n",
        "  fexec.map_reduce(map_preprocess, data_location, reduce_to_csv, obj_chunk_number=4) # 4 inferences per function\n",
        "  print(fexec.get_result())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-05-19 22:31:48,923 [INFO] lithops.config -- Lithops v2.3.3\n",
            "2021-05-19 22:31:48,936 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-de\n",
            "2021-05-19 22:31:48,939 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: eu-de - Namespace: Namespace-n4e-colab\n",
            "2021-05-19 22:31:48,940 [INFO] lithops.executors -- Serverless Executor created with ID: 2fbdc8-1\n",
            "2021-05-19 22:31:48,940 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID M000 - Selected Runtime: lithopscloud/ibmcf-python-v39 - 256MB\n",
            "2021-05-19 22:31:50,229 [INFO] lithops.job.job -- ExecutorID 2fbdc8-1 | JobID M000 - Uploading function and data - Total: 5.3KiB\n",
            "2021-05-19 22:31:50,640 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID M000 - Starting function invocation: map_preprocess() - Total: 24 activations\n",
            "2021-05-19 22:31:50,682 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID M000 - View execution logs at /tmp/lithops/logs/2fbdc8-1-M000.log\n",
            "2021-05-19 22:31:50,689 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID R000 - Selected Runtime: lithopscloud/ibmcf-python-v39 - 256MB\n",
            "2021-05-19 22:31:50,693 [INFO] lithops.job.job -- ExecutorID 2fbdc8-1 | JobID R000 - Uploading function and data - Total: 7.9KiB\n",
            "2021-05-19 22:31:51,287 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID R000 - Starting function invocation: reduce_to_csv() - Total: 1 activations\n",
            "2021-05-19 22:31:51,290 [INFO] lithops.invokers -- ExecutorID 2fbdc8-1 | JobID R000 - View execution logs at /tmp/lithops/logs/2fbdc8-1-R000.log\n",
            "2021-05-19 22:31:51,299 [INFO] lithops.wait -- ExecutorID 2fbdc8-1 - Getting results from functions\n",
            "2021-05-19 22:31:57,381 [INFO] lithops.executors -- ExecutorID 2fbdc8-1 - Cleaning temporary data\n",
            "reduced result to csv output and stored it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lithops logs poll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}