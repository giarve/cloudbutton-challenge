{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloudbutton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "source": [
        "Stage 2:  Data preprocessing stage to produce structured data in csv format also stored in Cloud Object Storage. As columns in the csv file we suggest date, geographic location, url, and sentiment analysis."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lithops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from custom_snscrape.twitter import Tweet\n",
        "\n",
        "# https://github.com/lithops-cloud/lithops/blob/master/docs/data_processing.md\n",
        "\n",
        "def map_preprocess(obj):\n",
        "\n",
        "    data_body_str = obj.data_stream.read().decode('utf-8')    \n",
        "    tweet = Tweet.from_json(data_body_str)\n",
        "\n",
        "\n",
        "    # if tweet.lang is not a supported language, translate it:\n",
        "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TranslationPipeline\n",
        "\n",
        "    #nlp = pipeline(\"ner\")\n",
        "    # https://huggingface.co/transformers/task_summary.html#named-entity-recognition\n",
        "\n",
        "\n",
        "    # this model is the best one, I tested the others available and they're quite bad...\n",
        "    classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\") # return_all_scores=True? to make better charts perhaps?\n",
        "    sentiment = classifier(tweet.content)\n",
        "\n",
        "    # if it has any attached image:\n",
        "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.ImageClassificationPipeline\n",
        "    # or text-generation??\n",
        "\n",
        "    \n",
        "    # TODO: Perhaps cast it all to string so we don't have to import the library inside reduce_to_csv\n",
        "    return { 'id': tweet.id,\n",
        "             'user': tweet.user,\n",
        "             'content': tweet.content,\n",
        "             'lang': tweet.lang,\n",
        "             'coordinates': tweet.coordinates,\n",
        "             'retweetCount': tweet.retweetCount,\n",
        "             'sentiment': sentiment[0]['label'] }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def reduce_to_csv(results, storage):\n",
        "    \n",
        "    # BUG: Because we are passing dataclasses from a pickled module inside map() function as its results, we must\n",
        "    # import something from the library that contains the dataclasses or lithops won't find it.\n",
        "    # If you don't believe me, remove this line :)\n",
        "    from custom_snscrape.twitter import Tweet\n",
        "    #############################################################################################################\n",
        "\n",
        "    column_names = results[0].keys()\n",
        "    csv_buffer = StringIO()\n",
        "\n",
        "    with StringIO() as csv_buffer:\n",
        "        dict_writer = csv.DictWriter(csv_buffer, column_names)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(results)\n",
        "\n",
        "        # TODO: replace all 'colab' with storage name from .lithops_config\n",
        "\n",
        "        storage.put_object(bucket='colab',\n",
        "                        key='test.csv',\n",
        "                        body=csv_buffer.getvalue())\n",
        "\n",
        "    return \"reduced result to cos://colab/test.csv output and stored it\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstiNpfTHqPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "7012ba8e-c793-4bc8-fcb8-7d02985a7823",
        "tags": []
      },
      "source": [
        "data_location = ['colab/#coronavirus/', 'colab/#sars/', 'colab/#covid/'] # The / at the end is important TODO: Avoid repetition? Reuse tags from stage 1 and prefix them with colab/\n",
        "\n",
        "# TODO: replace all 'colab' with storage name from .lithops_config\n",
        "\n",
        "with lithops.FunctionExecutor(runtime='gilarasa/lithops-cloudbutton-challenge-py3.9:0.5', runtime_memory=2048) as fexec:\n",
        "  # , log_level='debug'\n",
        "\n",
        "\n",
        "  #  TODO: should aggregate in chunks the tweets so that a single function can preprocess multiple tweets\n",
        "\n",
        "  fexec.map_reduce(map_preprocess, data_location, reduce_to_csv) # obj_chunk_number is used to divide the input string in chunks, so not to be used unless we store more than one tweet in each object (a list of tweets)\n",
        "  print(fexec.get_result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}